{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9272c",
   "metadata": {},
   "source": [
    "# Neural Networks with EMNIST\n",
    "\n",
    "Get familiar with using creating a neural network and adjusting parameters. \n",
    "This notebook uses the EMNIST dataset containing thousands of letters to build a machine learning model.\n",
    "\n",
    "This code is from a YouTube Tutorial by CrashCourse in their AI Series. \n",
    "The episode that this code is from is \"How to make an AI read your handwriting (LAB) \n",
    ": Crash Course Ai #5\" \n",
    "\n",
    "The video is linked below\n",
    "\n",
    "https://www.youtube.com/watch?v=6nGCGYWMObE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1391a6",
   "metadata": {},
   "source": [
    "## Import EMNIST library from git\n",
    "\n",
    "Access the EMNIST dataset from GitHub and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-mnist'...\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emnist) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emnist) (2.31.0)\n",
      "Collecting tqdm (from emnist)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "                                              0.0/77.1 kB ? eta -:--:--\n",
      "     -------------------------------------    71.7/77.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 77.1/77.1 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->emnist) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->emnist) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->emnist) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->emnist) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\taylo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->emnist) (0.4.6)\n",
      "Installing collected packages: tqdm, emnist\n",
      "Successfully installed emnist-0.0 tqdm-4.65.0\n",
      "Imported the EMNIST libraries we need!\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sorki/python-mnist\n",
    "!./python-mnist/bin/mnist_get_data.sh\n",
    "!pip3 install emnist\n",
    "from emnist import extract_training_samples\n",
    "\n",
    "print(\"Imported the EMNIST libraries we need!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fd276",
   "metadata": {},
   "source": [
    "## Train and Test\n",
    "\n",
    "Separate the data into training and testing data. \n",
    "\n",
    "Training data contains 60,000 data points and another 10,000 points to test with. \n",
    "The features are the pixel values and the targets is the letter the pixels represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8805cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading emnist.zip: 536MB [00:16, 33.4MB/s]                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted our samples and divided our training and testing data sets\n"
     ]
    }
   ],
   "source": [
    "# Grab the data from the OpenML website\n",
    "# X will be our images and y will be the labels\n",
    "X, y = extract_training_samples('letters')\n",
    "\n",
    "# Make sure that every pixel in all of the images is a value between 0 and 1\n",
    "X = X / 255.\n",
    "\n",
    "# Use the first 60000 instances as training and the next 10000 as testing\n",
    "X_train, X_test = X[:60000], X[60000:70000]\n",
    "y_train, y_test = y[:60000], y[60000:70000]\n",
    "\n",
    "# There is one other thing we need to do, we need to\n",
    "# record the number of samples in each dataset and the number of pixels in each image\n",
    "X_train = X_train.reshape(60000,784)\n",
    "X_test = X_test.reshape(10000,784)\n",
    "\n",
    "print(\"Extracted our samples and divided our training and testing data sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577ae20",
   "metadata": {},
   "source": [
    "### View sample image\n",
    "\n",
    "Open a random image from the training set and view it. \n",
    "Help get a better understanding of what the images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875c948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label: m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fcc67f84f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_index = 14000 # <<<<<  You can update this value to look at other images\n",
    "img = X_train[img_index]\n",
    "print(\"Image Label: \" + str(chr(y_train[img_index]+96)))\n",
    "plt.imshow(img.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25873f",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Create and evaluate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7579c",
   "metadata": {},
   "source": [
    "### 1-Layer, 50-neuron, 50-epoch\n",
    "\n",
    "A 1-layer neural network with 50 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd1ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created our first MLP network\n"
     ]
    }
   ],
   "source": [
    "# These two lines import the ML libraries we need\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# This creates our first MLP with 1 hidden layer with 50 neurons and sets it to run through the data 20 times\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=20, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "print(\"Created our first MLP network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbc950",
   "metadata": {},
   "source": [
    "The model's accuracy is 0.84 on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d719fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06351395\n",
      "Iteration 2, loss = 0.64844650\n",
      "Iteration 3, loss = 0.56103245\n",
      "Iteration 4, loss = 0.51987725\n",
      "Iteration 5, loss = 0.49182099\n",
      "Iteration 6, loss = 0.47301057\n",
      "Iteration 7, loss = 0.45839220\n",
      "Iteration 8, loss = 0.44603836\n",
      "Iteration 9, loss = 0.43479721\n",
      "Iteration 10, loss = 0.42809575\n",
      "Iteration 11, loss = 0.41639233\n",
      "Iteration 12, loss = 0.40782908\n",
      "Iteration 13, loss = 0.40548360\n",
      "Iteration 14, loss = 0.39965983\n",
      "Iteration 15, loss = 0.39296832\n",
      "Iteration 16, loss = 0.38883219\n",
      "Iteration 17, loss = 0.38393955\n",
      "Iteration 18, loss = 0.37948343\n",
      "Iteration 19, loss = 0.37307616\n",
      "Iteration 20, loss = 0.37166732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.886500\n",
      "Test set score: 0.840800\n"
     ]
    }
   ],
   "source": [
    "mlp1.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp1.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f0bd3",
   "metadata": {},
   "source": [
    "Create a confusion matrix to inspect the letters with the biggest difference between predicted and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea301d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's initialize a list with all the predicted values from the training set\n",
    "y_pred = mlp1.predict(X_test)\n",
    "\n",
    "# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00840aaf",
   "metadata": {},
   "source": [
    "View the cases where the model incorrectly identified 'l's and 'i's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47551fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 84 times that the letter i was predicted to be the letter l.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can change this to any letters that you think the neural network may have confused...\n",
    "predicted_letter = 'l'\n",
    "actual_letter = 'i'\n",
    "\n",
    "\n",
    "# This code counts all mistakes for the letters above\n",
    "mistake_list = []\n",
    "for i in range(len(y_test)):\n",
    "  if (y_test[i] == (ord(actual_letter) - 96) and y_pred[i] == (ord(predicted_letter) - 96)):\n",
    "    mistake_list.append(i)\n",
    "print(\"There were \" + str(len(mistake_list)) + \" times that the letter \" + actual_letter + \" was predicted to be the letter \" + predicted_letter + \".\")\n",
    "\n",
    "# Once we know how many mistakes were made, we can change this to see an image of a particular one\n",
    "mistake_to_show = 4 # <<< e.g., change this to 3 if you want to see the 4th mistake\n",
    "\n",
    "# This code checks that the number mistake you asked for can be shown and if so, displays an image of it\n",
    "if (len(mistake_list)> mistake_to_show):\n",
    "  img = X_test[mistake_list[mistake_to_show]]\n",
    "  plt.imshow(img.reshape((28,28)))\n",
    "else:\n",
    "  print(\"Couldn't show mistake number \" + str(mistake_to_show + 1) + \" because there were only \" + str(len(mistake_list)) + \" mistakes to show!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cc96d",
   "metadata": {},
   "source": [
    "### 5-Layer, 100-neuron, 50-epoch\n",
    "\n",
    "A 5-layer neural network with 100 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d564ad01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15599672\n",
      "Iteration 2, loss = 0.54834774\n",
      "Iteration 3, loss = 0.44784035\n",
      "Iteration 4, loss = 0.39865207\n",
      "Iteration 5, loss = 0.36308811\n",
      "Iteration 6, loss = 0.33567397\n",
      "Iteration 7, loss = 0.31904877\n",
      "Iteration 8, loss = 0.29485588\n",
      "Iteration 9, loss = 0.27846574\n",
      "Iteration 10, loss = 0.26938228\n",
      "Iteration 11, loss = 0.25802023\n",
      "Iteration 12, loss = 0.25788282\n",
      "Iteration 13, loss = 0.24842470\n",
      "Iteration 14, loss = 0.23652186\n",
      "Iteration 15, loss = 0.22631113\n",
      "Iteration 16, loss = 0.22402455\n",
      "Iteration 17, loss = 0.21873561\n",
      "Iteration 18, loss = 0.21993938\n",
      "Iteration 19, loss = 0.21469569\n",
      "Iteration 20, loss = 0.21052404\n",
      "Iteration 21, loss = 0.20297180\n",
      "Iteration 22, loss = 0.20881634\n",
      "Iteration 23, loss = 0.20199416\n",
      "Iteration 24, loss = 0.20136205\n",
      "Iteration 25, loss = 0.19870804\n",
      "Iteration 26, loss = 0.19468852\n",
      "Iteration 27, loss = 0.19161935\n",
      "Iteration 28, loss = 0.19082842\n",
      "Iteration 29, loss = 0.18859101\n",
      "Iteration 30, loss = 0.18797075\n",
      "Iteration 31, loss = 0.19353324\n",
      "Iteration 32, loss = 0.19176488\n",
      "Iteration 33, loss = 0.19011325\n",
      "Iteration 34, loss = 0.18515083\n",
      "Iteration 35, loss = 0.18550053\n",
      "Iteration 36, loss = 0.18208673\n",
      "Iteration 37, loss = 0.17424194\n",
      "Iteration 38, loss = 0.17742465\n",
      "Iteration 39, loss = 0.17507229\n",
      "Iteration 40, loss = 0.18140040\n",
      "Iteration 41, loss = 0.17806334\n",
      "Iteration 42, loss = 0.17466005\n",
      "Iteration 43, loss = 0.17764971\n",
      "Iteration 44, loss = 0.18040760\n",
      "Iteration 45, loss = 0.17872487\n",
      "Iteration 46, loss = 0.17533096\n",
      "Iteration 47, loss = 0.16693954\n",
      "Iteration 48, loss = 0.17500316\n",
      "Iteration 49, loss = 0.17623631\n",
      "Iteration 50, loss = 0.17240996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.949567\n",
      "Test set score: 0.889100\n"
     ]
    }
   ],
   "source": [
    "# Change some of the values in the below statement and re-run to see how they \n",
    "# affect performance!\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201e647",
   "metadata": {},
   "source": [
    "### 4-Layer, 100-neuron, 100-epoch\n",
    "\n",
    "A 4-layer neural network with 100 neurons and 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3bce74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.01379081\n",
      "Iteration 2, loss = 0.50392966\n",
      "Iteration 3, loss = 0.41916971\n",
      "Iteration 4, loss = 0.37202569\n",
      "Iteration 5, loss = 0.34055321\n",
      "Iteration 6, loss = 0.31400146\n",
      "Iteration 7, loss = 0.29491436\n",
      "Iteration 8, loss = 0.28594488\n",
      "Iteration 9, loss = 0.27441599\n",
      "Iteration 10, loss = 0.25836723\n",
      "Iteration 11, loss = 0.25526800\n",
      "Iteration 12, loss = 0.23921960\n",
      "Iteration 13, loss = 0.23775247\n",
      "Iteration 14, loss = 0.23193747\n",
      "Iteration 15, loss = 0.22274752\n",
      "Iteration 16, loss = 0.22223769\n",
      "Iteration 17, loss = 0.21942345\n",
      "Iteration 18, loss = 0.21173550\n",
      "Iteration 19, loss = 0.20685005\n",
      "Iteration 20, loss = 0.20574863\n",
      "Iteration 21, loss = 0.20002214\n",
      "Iteration 22, loss = 0.19976729\n",
      "Iteration 23, loss = 0.20194026\n",
      "Iteration 24, loss = 0.20843457\n",
      "Iteration 25, loss = 0.19250249\n",
      "Iteration 26, loss = 0.18499905\n",
      "Iteration 27, loss = 0.19201265\n",
      "Iteration 28, loss = 0.19276332\n",
      "Iteration 29, loss = 0.18838018\n",
      "Iteration 30, loss = 0.19103984\n",
      "Iteration 31, loss = 0.19097723\n",
      "Iteration 32, loss = 0.19098188\n",
      "Iteration 33, loss = 0.17579798\n",
      "Iteration 34, loss = 0.19177749\n",
      "Iteration 35, loss = 0.18558387\n",
      "Iteration 36, loss = 0.19258984\n",
      "Iteration 37, loss = 0.18173720\n",
      "Iteration 38, loss = 0.18148747\n",
      "Iteration 39, loss = 0.17486063\n",
      "Iteration 40, loss = 0.18227155\n",
      "Iteration 41, loss = 0.18639952\n",
      "Iteration 42, loss = 0.17711031\n",
      "Iteration 43, loss = 0.18194926\n",
      "Iteration 44, loss = 0.18717996\n",
      "Iteration 45, loss = 0.18691587\n",
      "Iteration 46, loss = 0.19462367\n",
      "Iteration 47, loss = 0.19771807\n",
      "Iteration 48, loss = 0.17625629\n",
      "Iteration 49, loss = 0.18136759\n",
      "Iteration 50, loss = 0.17235522\n",
      "Iteration 51, loss = 0.18891137\n",
      "Iteration 52, loss = 0.18420378\n",
      "Iteration 53, loss = 0.17998401\n",
      "Iteration 54, loss = 0.16743854\n",
      "Iteration 55, loss = 0.16908273\n",
      "Iteration 56, loss = 0.17800033\n",
      "Iteration 57, loss = 0.18895260\n",
      "Iteration 58, loss = 0.18933830\n",
      "Iteration 59, loss = 0.19030027\n",
      "Iteration 60, loss = 0.18910405\n",
      "Iteration 61, loss = 0.17686387\n",
      "Iteration 62, loss = 0.18031418\n",
      "Iteration 63, loss = 0.18298953\n",
      "Iteration 64, loss = 0.18054800\n",
      "Iteration 65, loss = 0.17355116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.947733\n",
      "Test set score: 0.882800\n"
     ]
    }
   ],
   "source": [
    "# Change some of the values in the below statement and re-run to see how they \n",
    "# affect performance!\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100,100,), max_iter=100, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201a244",
   "metadata": {},
   "source": [
    "### 6-Layer, 100-neuron, 50-epoch\n",
    "\n",
    "A 6-layer neural network with 100 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8959c5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.24960417\n",
      "Iteration 2, loss = 0.57582422\n",
      "Iteration 3, loss = 0.47712460\n",
      "Iteration 4, loss = 0.42119700\n",
      "Iteration 5, loss = 0.37742533\n",
      "Iteration 6, loss = 0.35172146\n",
      "Iteration 7, loss = 0.32579139\n",
      "Iteration 8, loss = 0.31228887\n",
      "Iteration 9, loss = 0.29365994\n",
      "Iteration 10, loss = 0.27529740\n",
      "Iteration 11, loss = 0.26802392\n",
      "Iteration 12, loss = 0.25236468\n",
      "Iteration 13, loss = 0.24922883\n",
      "Iteration 14, loss = 0.24197889\n",
      "Iteration 15, loss = 0.23588119\n",
      "Iteration 16, loss = 0.23155045\n",
      "Iteration 17, loss = 0.22522903\n",
      "Iteration 18, loss = 0.22597884\n",
      "Iteration 19, loss = 0.22145803\n",
      "Iteration 20, loss = 0.21808489\n",
      "Iteration 21, loss = 0.21427525\n",
      "Iteration 22, loss = 0.21071625\n",
      "Iteration 23, loss = 0.20072292\n",
      "Iteration 24, loss = 0.19746474\n",
      "Iteration 25, loss = 0.20185190\n",
      "Iteration 26, loss = 0.19215147\n",
      "Iteration 27, loss = 0.20082646\n",
      "Iteration 28, loss = 0.19751645\n",
      "Iteration 29, loss = 0.19395264\n",
      "Iteration 30, loss = 0.18737214\n",
      "Iteration 31, loss = 0.18573401\n",
      "Iteration 32, loss = 0.18003881\n",
      "Iteration 33, loss = 0.18615316\n",
      "Iteration 34, loss = 0.18445236\n",
      "Iteration 35, loss = 0.18164565\n",
      "Iteration 36, loss = 0.18307255\n",
      "Iteration 37, loss = 0.18665968\n",
      "Iteration 38, loss = 0.18472613\n",
      "Iteration 39, loss = 0.18549334\n",
      "Iteration 40, loss = 0.17997282\n",
      "Iteration 41, loss = 0.18307871\n",
      "Iteration 42, loss = 0.18297673\n",
      "Iteration 43, loss = 0.17791744\n",
      "Iteration 44, loss = 0.17804395\n",
      "Iteration 45, loss = 0.17035899\n",
      "Iteration 46, loss = 0.17000799\n",
      "Iteration 47, loss = 0.18090037\n",
      "Iteration 48, loss = 0.18904882\n",
      "Iteration 49, loss = 0.18302817\n",
      "Iteration 50, loss = 0.18364420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.944833\n",
      "Test set score: 0.885300\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100,100), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ebb86",
   "metadata": {},
   "source": [
    "### 3-Layer, 100-neuron, 50-epoch\n",
    "\n",
    "A 3-layer neural network with 100 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cf6acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.94567123\n",
      "Iteration 2, loss = 0.47719782\n",
      "Iteration 3, loss = 0.39525794\n",
      "Iteration 4, loss = 0.35000380\n",
      "Iteration 5, loss = 0.32218087\n",
      "Iteration 6, loss = 0.29480242\n",
      "Iteration 7, loss = 0.27962279\n",
      "Iteration 8, loss = 0.26156196\n",
      "Iteration 9, loss = 0.24643216\n",
      "Iteration 10, loss = 0.23951834\n",
      "Iteration 11, loss = 0.23188084\n",
      "Iteration 12, loss = 0.22801692\n",
      "Iteration 13, loss = 0.21724162\n",
      "Iteration 14, loss = 0.21456462\n",
      "Iteration 15, loss = 0.20922584\n",
      "Iteration 16, loss = 0.20420448\n",
      "Iteration 17, loss = 0.20105150\n",
      "Iteration 18, loss = 0.19873714\n",
      "Iteration 19, loss = 0.19204121\n",
      "Iteration 20, loss = 0.19160648\n",
      "Iteration 21, loss = 0.20064392\n",
      "Iteration 22, loss = 0.19757599\n",
      "Iteration 23, loss = 0.18845715\n",
      "Iteration 24, loss = 0.18503059\n",
      "Iteration 25, loss = 0.17863643\n",
      "Iteration 26, loss = 0.18348528\n",
      "Iteration 27, loss = 0.18410600\n",
      "Iteration 28, loss = 0.18448267\n",
      "Iteration 29, loss = 0.18810097\n",
      "Iteration 30, loss = 0.18511120\n",
      "Iteration 31, loss = 0.18215740\n",
      "Iteration 32, loss = 0.18417196\n",
      "Iteration 33, loss = 0.18261394\n",
      "Iteration 34, loss = 0.17538901\n",
      "Iteration 35, loss = 0.18056121\n",
      "Iteration 36, loss = 0.18102824\n",
      "Iteration 37, loss = 0.17494836\n",
      "Iteration 38, loss = 0.19016246\n",
      "Iteration 39, loss = 0.19219674\n",
      "Iteration 40, loss = 0.19221844\n",
      "Iteration 41, loss = 0.17128911\n",
      "Iteration 42, loss = 0.17993551\n",
      "Iteration 43, loss = 0.18926259\n",
      "Iteration 44, loss = 0.18705908\n",
      "Iteration 45, loss = 0.17629385\n",
      "Iteration 46, loss = 0.17829538\n",
      "Iteration 47, loss = 0.16909959\n",
      "Iteration 48, loss = 0.17736057\n",
      "Iteration 49, loss = 0.19200277\n",
      "Iteration 50, loss = 0.17393551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.941133\n",
      "Test set score: 0.878200\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b234e8",
   "metadata": {},
   "source": [
    "### 2-Layer, 100-neuron, 50-epoch\n",
    "\n",
    "A 2-layer neural network with 100 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb55c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.92341903\n",
      "Iteration 2, loss = 0.47113924\n",
      "Iteration 3, loss = 0.38867102\n",
      "Iteration 4, loss = 0.34033358\n",
      "Iteration 5, loss = 0.31797103\n",
      "Iteration 6, loss = 0.29433554\n",
      "Iteration 7, loss = 0.27297637\n",
      "Iteration 8, loss = 0.25533497\n",
      "Iteration 9, loss = 0.24492291\n",
      "Iteration 10, loss = 0.23219130\n",
      "Iteration 11, loss = 0.22821158\n",
      "Iteration 12, loss = 0.21875247\n",
      "Iteration 13, loss = 0.20788412\n",
      "Iteration 14, loss = 0.19990241\n",
      "Iteration 15, loss = 0.20067924\n",
      "Iteration 16, loss = 0.19264319\n",
      "Iteration 17, loss = 0.18396688\n",
      "Iteration 18, loss = 0.18335998\n",
      "Iteration 19, loss = 0.18505259\n",
      "Iteration 20, loss = 0.17711433\n",
      "Iteration 21, loss = 0.17423039\n",
      "Iteration 22, loss = 0.17349865\n",
      "Iteration 23, loss = 0.17724000\n",
      "Iteration 24, loss = 0.17587735\n",
      "Iteration 25, loss = 0.16150978\n",
      "Iteration 26, loss = 0.15711366\n",
      "Iteration 27, loss = 0.16454959\n",
      "Iteration 28, loss = 0.15273586\n",
      "Iteration 29, loss = 0.16872881\n",
      "Iteration 30, loss = 0.16765205\n",
      "Iteration 31, loss = 0.16283669\n",
      "Iteration 32, loss = 0.15658654\n",
      "Iteration 33, loss = 0.15662268\n",
      "Iteration 34, loss = 0.17038016\n",
      "Iteration 35, loss = 0.15751889\n",
      "Iteration 36, loss = 0.15406750\n",
      "Iteration 37, loss = 0.17095200\n",
      "Iteration 38, loss = 0.16530475\n",
      "Iteration 39, loss = 0.16108390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.948417\n",
      "Test set score: 0.873600\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d233e",
   "metadata": {},
   "source": [
    "### 2-Layer, 100-neuron, 100-epoch\n",
    "\n",
    "A 2-layer neural network with 100 neurons and 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238c773b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.92341903\n",
      "Iteration 2, loss = 0.47113924\n",
      "Iteration 3, loss = 0.38867102\n",
      "Iteration 4, loss = 0.34033358\n",
      "Iteration 5, loss = 0.31797103\n",
      "Iteration 6, loss = 0.29433554\n",
      "Iteration 7, loss = 0.27297637\n",
      "Iteration 8, loss = 0.25533497\n",
      "Iteration 9, loss = 0.24492291\n",
      "Iteration 10, loss = 0.23219130\n",
      "Iteration 11, loss = 0.22821158\n",
      "Iteration 12, loss = 0.21875247\n",
      "Iteration 13, loss = 0.20788412\n",
      "Iteration 14, loss = 0.19990241\n",
      "Iteration 15, loss = 0.20067924\n",
      "Iteration 16, loss = 0.19264319\n",
      "Iteration 17, loss = 0.18396688\n",
      "Iteration 18, loss = 0.18335998\n",
      "Iteration 19, loss = 0.18505259\n",
      "Iteration 20, loss = 0.17711433\n",
      "Iteration 21, loss = 0.17423039\n",
      "Iteration 22, loss = 0.17349865\n",
      "Iteration 23, loss = 0.17724000\n",
      "Iteration 24, loss = 0.17587735\n",
      "Iteration 25, loss = 0.16150978\n",
      "Iteration 26, loss = 0.15711366\n",
      "Iteration 27, loss = 0.16454959\n",
      "Iteration 28, loss = 0.15273586\n",
      "Iteration 29, loss = 0.16872881\n",
      "Iteration 30, loss = 0.16765205\n",
      "Iteration 31, loss = 0.16283669\n",
      "Iteration 32, loss = 0.15658654\n",
      "Iteration 33, loss = 0.15662268\n",
      "Iteration 34, loss = 0.17038016\n",
      "Iteration 35, loss = 0.15751889\n",
      "Iteration 36, loss = 0.15406750\n",
      "Iteration 37, loss = 0.17095200\n",
      "Iteration 38, loss = 0.16530475\n",
      "Iteration 39, loss = 0.16108390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.948417\n",
      "Test set score: 0.873600\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,), max_iter=100, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865efb39",
   "metadata": {},
   "source": [
    "### 2-Layer, 200-neuron, 50-epoch\n",
    "\n",
    "A 2-layer neural network with 200 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70aeaf71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84157388\n",
      "Iteration 2, loss = 0.41124289\n",
      "Iteration 3, loss = 0.32685027\n",
      "Iteration 4, loss = 0.27822481\n",
      "Iteration 5, loss = 0.24477098\n",
      "Iteration 6, loss = 0.21631529\n",
      "Iteration 7, loss = 0.19502451\n",
      "Iteration 8, loss = 0.17756042\n",
      "Iteration 9, loss = 0.15924957\n",
      "Iteration 10, loss = 0.14356044\n",
      "Iteration 11, loss = 0.13818683\n",
      "Iteration 12, loss = 0.12896495\n",
      "Iteration 13, loss = 0.11841989\n",
      "Iteration 14, loss = 0.11316019\n",
      "Iteration 15, loss = 0.10580086\n",
      "Iteration 16, loss = 0.10150027\n",
      "Iteration 17, loss = 0.09859888\n",
      "Iteration 18, loss = 0.09059813\n",
      "Iteration 19, loss = 0.09015529\n",
      "Iteration 20, loss = 0.08736004\n",
      "Iteration 21, loss = 0.08290141\n",
      "Iteration 22, loss = 0.08421612\n",
      "Iteration 23, loss = 0.07926881\n",
      "Iteration 24, loss = 0.07811772\n",
      "Iteration 25, loss = 0.08379352\n",
      "Iteration 26, loss = 0.08303772\n",
      "Iteration 27, loss = 0.09512353\n",
      "Iteration 28, loss = 0.07959204\n",
      "Iteration 29, loss = 0.08386981\n",
      "Iteration 30, loss = 0.08447192\n",
      "Iteration 31, loss = 0.08153120\n",
      "Iteration 32, loss = 0.08627134\n",
      "Iteration 33, loss = 0.08533315\n",
      "Iteration 34, loss = 0.08060735\n",
      "Iteration 35, loss = 0.09593161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.969917\n",
      "Test set score: 0.890900\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,200,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f8a35",
   "metadata": {},
   "source": [
    "### 3-Layer, 200-neuron, 50-epoch\n",
    "\n",
    "A 3-layer neural network with 200 neurons and 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97fce1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85006833\n",
      "Iteration 2, loss = 0.42132711\n",
      "Iteration 3, loss = 0.34014313\n",
      "Iteration 4, loss = 0.28538280\n",
      "Iteration 5, loss = 0.25113998\n",
      "Iteration 6, loss = 0.23027039\n",
      "Iteration 7, loss = 0.20491522\n",
      "Iteration 8, loss = 0.18557848\n",
      "Iteration 9, loss = 0.17633386\n",
      "Iteration 10, loss = 0.16720570\n",
      "Iteration 11, loss = 0.15510447\n",
      "Iteration 12, loss = 0.14226318\n",
      "Iteration 13, loss = 0.13870009\n",
      "Iteration 14, loss = 0.13507830\n",
      "Iteration 15, loss = 0.13196642\n",
      "Iteration 16, loss = 0.13615208\n",
      "Iteration 17, loss = 0.12941351\n",
      "Iteration 18, loss = 0.12493503\n",
      "Iteration 19, loss = 0.12010820\n",
      "Iteration 20, loss = 0.11158571\n",
      "Iteration 21, loss = 0.12005928\n",
      "Iteration 22, loss = 0.11618404\n",
      "Iteration 23, loss = 0.12492159\n",
      "Iteration 24, loss = 0.11358622\n",
      "Iteration 25, loss = 0.11479737\n",
      "Iteration 26, loss = 0.11409521\n",
      "Iteration 27, loss = 0.11565854\n",
      "Iteration 28, loss = 0.11527723\n",
      "Iteration 29, loss = 0.12009333\n",
      "Iteration 30, loss = 0.11039331\n",
      "Iteration 31, loss = 0.10923919\n",
      "Iteration 32, loss = 0.10390549\n",
      "Iteration 33, loss = 0.11187356\n",
      "Iteration 34, loss = 0.11584595\n",
      "Iteration 35, loss = 0.10797678\n",
      "Iteration 36, loss = 0.11416854\n",
      "Iteration 37, loss = 0.13129034\n",
      "Iteration 38, loss = 0.12727238\n",
      "Iteration 39, loss = 0.11516022\n",
      "Iteration 40, loss = 0.11273762\n",
      "Iteration 41, loss = 0.10406823\n",
      "Iteration 42, loss = 0.11787838\n",
      "Iteration 43, loss = 0.10931539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.965233\n",
      "Test set score: 0.891000\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,200,200), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp2.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
